# -*- coding: utf-8 -*-
"""Resume_Parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m2an8TPSGRLPXTYcA5Q0lNlWQcwhCP4j

Step 1: Install Required Libraries
"""

!pip install transformers sentence-transformers nltk pymupdf
!python -m nltk.downloader stopwords

"""Step 2: Upload Resume File (.pdf or .txt)"""

from google.colab import files
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

"""Step 3: Extract Text from File"""

import fitz  # PyMuPDF

def extract_text(file_path):
    if file_path.endswith(".pdf"):
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()
        return text
    elif file_path.endswith(".txt"):
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        return ""

""" Step 4: Clean the Resume"""

import re

def clean_resume_text(text):
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'[â€¢\-\*]+', ' ', text)  # remove bullets
    text = re.sub(r'http\S+|www\S+', '', text)  # remove URLs
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip()

"""Step 5: Load NER Pipeline (Roberta)"""

from transformers import pipeline

ner_pipeline = pipeline("ner", model="Jean-Baptiste/roberta-large-ner-english", grouped_entities=True)

"""Step 6: Extract Entities & Info"""

from collections import defaultdict
from sentence_transformers import SentenceTransformer, util

# Load sentence embedding model for fuzzy matching
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Skills database
SKILLS_DB = ['python', 'java', 'sql', 'nlp', 'machine learning', 'deep learning', 'pandas', 'numpy', 'tensorflow', 'flask']

# Degrees and keywords
degree_keywords = ['b.tech', 'bachelor', 'm.tech', 'mba', 'ph.d', 'msc', 'b.sc', 'computer science', 'engineering']
exp_keywords = ['years', 'months', 'experience', 'worked', 'employment', 'intern', 'role']

def extract_structured_info(text):
    clean_text = clean_resume_text(text)
    ner_results = ner_pipeline(clean_text)
    structured = {
        "Name": None,
        "Institutions": set(),
        "Degree": set(),
        "Work_Experience": set(),
        "Skills": set()
    }

    # Name from top lines
    top_lines = '\n'.join(text.split('\n')[:5])
    top_ner = ner_pipeline(top_lines)
    for ent in top_ner:
        if ent['entity_group'] == 'PER':
            structured["Name"] = ent['word']
            break

    # Institutions via NER
    for item in ner_results:
        if item['entity_group'] == "ORG":
            structured["Institutions"].add(item['word'])

    # Degree keywords
    for keyword in degree_keywords:
        if keyword in clean_text.lower():
            structured["Degree"].add(keyword.title())

    # Experience (regex-based)
    exp_matches = re.findall(r'\d+\+?\s*(years?|months?)', clean_text.lower())
    for match in exp_matches:
        structured["Work_Experience"].add(match)

    # Experience context (lines with experience keywords)
    for line in text.split('\n'):
        if any(k in line.lower() for k in exp_keywords):
            structured["Work_Experience"].add(line.strip())

    # Fuzzy Skill Matching using embeddings
    resume_embedding = embedding_model.encode(clean_text, convert_to_tensor=True)

    for skill in SKILLS_DB:
        score = util.cos_sim(
            embedding_model.encode(skill, convert_to_tensor=True),
            resume_embedding
        )[0][0].item()

        if score > 0.3 or skill.lower() in clean_text.lower():
            structured["Skills"].add(skill)

    # Convert sets to lists
    for key in structured:
        if isinstance(structured[key], set):
            structured[key] = list(structured[key])

    return structured

"""Step 7: Run the Parser"""

resume_text = extract_text(file_name)
info = extract_structured_info(resume_text)

import json
print(json.dumps(info, indent=4))

